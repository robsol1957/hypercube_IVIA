---
title: "example"
author: "Rob Solomon"
date: "2026-01-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(tidyverse)
library(stringr)
library(forecast)

library(TTR)
```

# A Simple Stock Management Algorithm

## Six basic Steps

#### 1 Articulate Stock Control Algorithm

#### 2 Estimate the average daily sales

#### 3 Estimate the distribution of Lead Times

#### 4 use Simulation to tune stock control Algorithm

#### 5 Implement Stock Control system

#### 6 Monitor Performance and fine tune

This report will give a simple example of the first 4 steps.

## Step 1 Articulate Stock Control Algorithm

Re-stocking could be conducted on a daily basis. Orders would be calculated using the following equation

SOH_at_end_of_day = opening_stock + received_stock - sales

potential_orders = Target_stock_level - SOH_at_end_of_day - back_orders

new_order = max(potential_orders+1-min_order_size,0)

opening_stock = SOH_at_end_of_day (day-1)

The key discretionary parameter within this algorithm is target_stock_level.

This parameter will be a function of several factors

1.The distribution of daily sales.

2.The distribution of lead times.

3.The acceptable probability of a stock out.

It is proposed thatinitially simulation be used t estimate target stocks. In time this may be simplified if systematic behahiour is observed.

## Step 2 Estimate Average Daily Sales

The proposal is to use an optimised Exponentially Weighted Moving Average (EWMA) to determine the estimate of the most recent data.

The code to achieve this is shown below.

### Initial Data Wrangling

```{r, warning=F,message=F}
Dim_product_updated <- read_csv("data/Dim_product_updated.csv")
Dim_store <- read_csv("data/Dim_store.csv")
pos_transactions <- read_csv("data/fact_pos_alergy_cw_nsw_fs2025.csv")
pos_transactions <- pos_transactions %>% 
  rename(entity_id=eid)
pos_transactions <- left_join(pos_transactions,Dim_store %>% select(entity_id,name))
pos_transactions <- left_join(pos_transactions,Dim_product_updated %>% select(pfc,apn,pack_long_name))
pos_transactions <- pos_transactions %>% 
  mutate(GTIN = str_pad(apn,14,side="left"))

date_sequence_daily <- data.frame(sale_date=seq(from = as.Date('2025-01-01'), to = as.Date('2025-12-12'), by = "day"))
total_sales_by_product_by_store_day <- pos_transactions %>% 
  group_by(name,GTIN,pack_long_name,sale_date) %>% 
  summarise(sales =sum(unit),transactions=n()) %>% 
  arrange(-sales)
total_sales_by_product_by_store <- total_sales_by_product_by_store_day %>% 
  group_by(name,GTIN,pack_long_name) %>% 
  summarise(sales =sum(sales),transactions=sum(transactions)) %>% 
  arrange(-sales)

# select a single line item for a store

selected_series <- total_sales_by_product_by_store_day %>% 
  filter(name =="Chem Warehouse Castle Towers",
         GTIN == "09300607490381")
```


### Build Forecast

```{r, warning=F,message=F}
# Generate the EWMA fit of a time series (x), given the smoothing parameter lambda
ewma.filter <- function (x, lambda) {
  init <- round(0.5 / lambda)
  init <- mean(x[1:init])
  c(stats::filter(x * lambda, 1 - lambda, "recursive", init = init))
}
# Generate the partial autocorrelation coefficients of a timeseries (vec)
get_pacf <- function(vec,plot = T) {
  # generate PACF vector
  pacf <- pacf(vec,plot=plot)
  abspacf <- abs(pacf$acf)
  max_acf <- max(abspacf)
  # locate the lag for the maximum pacf value
  lag <- match(max_acf,abspacf)
  #convert pacf values to significance ~ (<1 not significant at 95%, >=1 significant at 95%)
  max_acf <- max(abs(pacf$acf)) / (2 / sqrt(length(vec)))
  list(pacf = pacf, max_acf = max_acf,max_lag = lag)
}

#Generate and test the validity of the forecast for a data frame containing time series data in the column sales
# Train model of first "rows" of rows of data
# Test the model by generating "forecats" number of forecasts into the futue and comparing with actuals.

fit_forecast <- function(df, rows, forecasts) {
  comment = ""
  # Ensure all days are present, impute 0 sales on missing days
  date_sequence_daily <- data.frame(sale_date = seq(
    from = min(df$sale_date),
    to = max(df$sale_date),
    by = "day"
  ))
  df <- left_join(date_sequence_daily, df, join_by(sale_date == sale_date))
  df$sales[is.na(df$sales)] <- 0
  #
  if (nrow(df) > (rows + forecasts)) { # if there is enough data to generate and test the forecast
   
    # split the data into train and test
    test_end = rows
    train <- df[1:test_end, ]
    test <- df[(test_end + 1):(test_end + forecasts), ]
    # Use ets function from "forecast" package to produce an optimised forecast model
    fit <- ets(train$sales, model = "ANN")
    lambda <- fit$par[1]
    comment = paste0(comment, "lambda :", round(lambda, digits = 3))
    train$ewma <- as.numeric(fit$fitted)
    train$resid <- as.numeric(fit$residuals)
    train$last_Std_est <- as.numeric(roll::roll_sd(train$resid, 30))
    fcast <- forecast(fit, h = forecasts)
    test$forecast <- as.numeric(fcast$mean)
    test$resid <- as.numeric(test$forecast - test$sales)
    test <- test %>%
      mutate(cum_resid = cumsum(test$resid))
    # Test for normality of both raw sales and for residuals (if p.value < .01 then near normal)
    shapero_sales <- shapiro.test(train$sales)
    shapero_resid <- shapiro.test(train$resid)
    # Test for independence of both raw sales and rewsiduals (if max(PACF) less than 1 then independent)
    pacf_sales <- get_pacf(train$sales,plot=F)
    pacf_resid <- get_pacf(train$resid,plot=F)
    
    ret <- list(
      succeed = T,
      test = test,
      train = train,
      model = fit,
      rmse = sqrt(sum(test$resid^2)),
      shapero_sales_p_value = shapero_sales$p.value,
      max_pacf_sales= pacf_sales$max_acf,
      max_Lag_sales = pacf_sales$max_lag,
      shapero_resid_p_value = shapero_resid$p.value,
      max_pacf_resid = pacf_resid$max_acf,
      max_Lag_resid = pacf_resid$max_lag,
      comment = comment
    )
  } else {
    ret <- list(succeed = F, comment = "Not enough data to forecast")
  }
  ret
}

# Fit the model
forecast <- fit_forecast(df = selected_series,rows=300,forecasts=10)

train <- forecast$train
test <- forecast$test 
  
combined <- full_join(train %>% select(sale_date,sales,ewma,resid,name,pack_long_name),
                      test %>% select(sale_date,sales,forecast,resid))
combined <- combined %>% 
  mutate(is_fcast = ifelse(is.na(forecast),"Actual","Forecast"))

```

### summary of the model

```{r echo=F}
summary(forecast$model)

cat("\nTest for normality \n")
cat(paste0("shapero p value of residuals :",formatC(forecast$shapero_resid_p_value,digits=2,format = "e")),"\n")
if(forecast$shapero_resid_p_value<0.04) {
  print("Residuals are normally distributed")
} else {
  print("residuals are non normal")
}
cat("\nTest for independance \n")
print(paste0("Maximum pacf value of residuals :",round(forecast$max_pacf_resid,2)))
if(forecast$max_pacf_resid < 1) {
  print("Residuals are independant")
} else {
  print("Autocorrelation Exists")
  print(paste0("Maximum pacf value occurs at lag :",forecast$max_Lag_resid, " days" ))
}

cat(paste0("\nEstimate of standard deviation at end of train period :",round(train$last_Std_est[nrow(train)],2)," units"))
```

### Plot of Trend and Residuals

```{r, warning=F,message=F, echo=FALSE}
combined %>% ggplot(aes(x=sale_date,y=sales,colour = is_fcast))+ geom_point() +
  geom_line(aes(y=ewma),colour='red') + geom_line(aes(y=forecast),colour="green")+
  labs(title = paste0("History and Forecast"),
       subtitle = paste0(combined$pack_long_name[1]," sold from ",combined$name[1],"\n dotted = actual, line = model"),
       x = "Sales Date",
       y ="Daily Sales (units)")

combined %>% ggplot(aes(x=sale_date,y=resid,colour = is_fcast))+ geom_point() +
  # geom_line(aes(y=ewma),colour='red') + geom_line(aes(y=forecast),colour="green")+
  labs(title = paste0("Residuals"),
       subtitle = paste0(combined$pack_long_name[1]," sold from ",combined$name[1]),
       x = "Sales Date",
       y ="Residual (units)")

```

## Step 3 Estimate the distribution of Lead Times

### Estimate Distribution of Daily Sales

I have not determined the distribution of lead times from the data but for the purposes of this analysis I have assumed the following distribution.

```{r}

lead_times <- data.frame(days = c(1:3), prob = c(.7, .2, .1))
avg <- sum(lead_times$days*lead_times$prob)
lead_times %>% ggplot(aes(x=days,y=prob))+ geom_col(fill="blue",colour="blue",alpha =.3)+
  scale_y_continuous(labels = scales::percent) +
  annotate("label",x=mean(lead_times$days),y=max(lead_times$prob),
           label = paste0("Average lead time :", avg," days"),
           fill="pink",
           colour="black") +
  labs(title = "Example of lead time distribution used in simulation",x="lead time (days)",y="Probability of achieving (%")

```

## Step 4 use Simulation to tune stock control Algorithm

For the purposes of the simulation, the daily arrivals is assumed to follow the Poisson Distribution, which is the theoretical distribution of the number of independently occurring events with uniform probability which will occur within a given time. 

The standard deviation of the Poisson distribution is given by the square root of the average number of events.

Using this feature as well as the assumed probability distribution for lead times allows the stochastic simulation of the algorithm from step 1 once a value of target stocks is chosen.  By running a series of simulation for a range of target stocks the observed sales performance can be determined.  For the sake of this example,  the sales performance for `r train$pack_long_name[1]` sold from `r train$name[1]` was determined for a range of target stocks between 2 and 25.  This sequence was repeated 10 times.

Running this simulation and plotting results yields the following trend.

```{r,message=F,warning=F,echo=F}
avg_daily_sales <- combined$forecast[nrow(combined)]
all_results <- read_csv("all_results_6.28167071280997_min_batch_1.csv") %>% 
  mutate(pct = sales/theoretic_sales)

all_results %>% ggplot(aes(x = target_stock, y = sales)) + geom_point(colour="blue")+
  geom_smooth(colour="blue",fill="blue",alpha=0.1)+
  geom_point(aes(y=lost_sales),colour="red")+
  geom_smooth(aes(y=lost_sales),colour="red",fill="red",alpha=0.1)+
  geom_line(aes(y=theoretic_sales),colour="black")+
  labs(title="Impact of Target Stock on Sales performance",
       subtitle = "Blue - Sales : Red - Lost Sales : Black -Expected Sales",
       x="Target Stock Level (units)",
       y= "Sales (units)")+
  annotate("label",x=max(all_results$target_stock/2),y=max(all_results$sales/2),
           label = paste0("avg_sales :", round(avg_daily_sales,2)),
           fill="pink",colour="black"
           )



goal=0.95

```

The target stock level required to achieve 95% of available sales can be determined by reversing the axes, fitting a curve to the data and using the fitted model to back calculate the required target stocks.

In the example below, the target stock level to achieve a 'round(10*goal,1)` sales reliability is determined.

```{r}

df <- all_results %>% 
  mutate(pct = sales/max(sales)) 
loess_m <- loess(df$target_stock~df$pct)
df$fit <- predict(loess_m,newdata = df$pct)


yg <- predict(loess_m,goal)
path <- data.frame(x=c(min(df$pct),goal,goal),y=c(yg,yg,0))
df %>% ggplot(aes(x=pct,y=target_stock))+ geom_point()+
  geom_line(aes(y=fit),colour="red")+
  annotate("label",x=min(df$pct+.2),y=yg+1,
           label = paste0("target stock level :", round(yg,2)),
           fill="pink",colour="black")+
  geom_line(data=path,aes(x=x,y=y))+
  labs(title = "Target Stock Required to Achieve Sales Reliability",
       subtitle = paste0("Avg daily sales :",round(avg_daily_sales,2)," Target reliability :",round(100*goal,1),"%, Target Stock Level :",ceiling(yg)),
       x="Target Sales Reliability (%)",
       y= "Target Stock (units)") +
  scale_x_continuous(labels = scales::percent)

 

```
